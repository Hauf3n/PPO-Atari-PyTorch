{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "device = torch.device(\"cuda:0\")\n",
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO_Network(nn.Module):\n",
    "    def __init__(self, in_channels, num_actions):\n",
    "        super().__init__()\n",
    "        \n",
    "        network = [\n",
    "            nn.Linear(in_channels,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, num_actions + 1)\n",
    "        ]\n",
    "        \n",
    "        self.network = nn.Sequential(*network)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        policy, value = torch.split(self.network(x),(num_actions, 1), dim=1)\n",
    "        policy = F.softmax(policy, dim=1)\n",
    "        return policy, value\n",
    "    \n",
    "class PPO_Agent(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, num_actions):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.num_actions = num_actions\n",
    "        self.network = PPO_Network(in_channels, num_actions)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        policy, value = self.network(x)\n",
    "        return policy, value\n",
    "    \n",
    "    def select_action(self, policy):\n",
    "        return np.random.choice(range(self.num_actions) , 1, p=policy)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    \n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        f = open(f\"{self.filename}.csv\", \"w\")\n",
    "        f.close()\n",
    "        \n",
    "    def log(self, msg):\n",
    "        f = open(f\"{self.filename}.csv\", \"a+\")\n",
    "        f.write(f\"{msg}\\n\")\n",
    "        f.close()\n",
    "        \n",
    "total_steps = 0\n",
    "class Env_Runner:\n",
    "    \n",
    "    def __init__(self, env_name, agent, logger_folder):\n",
    "        super().__init__\n",
    "        \n",
    "        \n",
    "        self.env = gym.make(env_name)\n",
    "        self.agent = agent\n",
    "        \n",
    "        self.logger = Logger(f'{logger_folder}/training_info')\n",
    "        self.logger.log(\"training_step, return\")\n",
    "        \n",
    "        self.ob = self.env.reset()\n",
    "        self.Return = 0\n",
    "        \n",
    "    def run(self, steps):\n",
    "        global total_steps\n",
    "        \n",
    "        obs = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "        values = []\n",
    "        action_prob = []\n",
    "        \n",
    "        for step in range(steps):\n",
    "            \n",
    "            self.ob = torch.tensor(self.ob).to(device).to(dtype)\n",
    "            policy, value = self.agent(self.ob.unsqueeze(0))\n",
    "            action = self.agent.select_action(policy.detach().cpu().numpy()[0])\n",
    "            \n",
    "            obs.append(self.ob)\n",
    "            actions.append(action)\n",
    "            values.append(value.detach())\n",
    "            action_prob.append(policy[0,action].detach())\n",
    "            \n",
    "            self.ob, r, done, info = self.env.step(action)\n",
    "            self.Return += r\n",
    "            \n",
    "            if done: # environment reset\n",
    "                self.ob = self.env.reset()\n",
    "                self.logger.log(f'{total_steps+step},{self.Return}')\n",
    "                print(\"Return:\",self.Return)\n",
    "                self.Return = 0\n",
    "            \n",
    "            rewards.append(r)\n",
    "            dones.append(done)\n",
    "            \n",
    "        total_steps += steps\n",
    "                                    \n",
    "        return [obs, actions, rewards, dones, values, action_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "lam = 0.95\n",
    "def compute_advantage_and_value_targets(rewards, values, dones):\n",
    "    \n",
    "    advantage_values = []\n",
    "    old_adv_t = torch.tensor(0.0).to(device)\n",
    "    \n",
    "    value_targets = []\n",
    "    old_value_target = values[-1]\n",
    "    \n",
    "    for t in reversed(range(len(rewards)-1)):\n",
    "        \n",
    "        if dones[t]:\n",
    "            old_adv_t = torch.tensor(0.0).to(device)\n",
    "        \n",
    "        # ADV\n",
    "        delta_t = rewards[t] + (gamma*(values[t+1])*int(not dones[t+1])) - values[t]\n",
    "        \n",
    "        A_t = delta_t + gamma*lam*old_adv_t\n",
    "        advantage_values.append(A_t[0])\n",
    "        \n",
    "        old_adv_t = delta_t + gamma*lam*old_adv_t\n",
    "        \n",
    "        # VALUE TARGET\n",
    "        value_target = rewards[t] + gamma*old_value_target*int(not dones[t+1])\n",
    "        value_targets.append(value_target[0])\n",
    "        \n",
    "        old_value_target = value_target\n",
    "    \n",
    "    advantage_values.reverse()\n",
    "    value_targets.reverse()\n",
    "    \n",
    "    return advantage_values, value_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch_DataSet(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, obs, actions, adv, v_t, old_action_prob):\n",
    "        super().__init__()\n",
    "        self.obs = obs\n",
    "        self.actions = actions\n",
    "        self.adv = adv\n",
    "        self.v_t = v_t\n",
    "        self.old_action_prob = old_action_prob\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.obs.shape[0]\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.obs[i],self.actions[i],self.adv[i],self.v_t[i],self.old_action_prob[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder to save networks, csv, hyperparameter\n",
    "folder_name = time.asctime(time.gmtime()).replace(\" \",\"_\").replace(\":\",\"_\")\n",
    "os.mkdir(folder_name)\n",
    "\n",
    "env_name = \"Acrobot-v1\"#\"CartPole-v0\"\n",
    "env = gym.make(env_name)\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "epochs = 4\n",
    "T = 65\n",
    "minibatch_size = 32\n",
    "lr = 1e-3\n",
    "eps = 0.1\n",
    "c1 = 0.1\n",
    "\n",
    "agent = PPO_Agent(obs_dim, num_actions).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=lr)\n",
    "actors = 4\n",
    "env_runners = [Env_Runner(env_name, agent, folder_name) for i in range(actors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iterations = 1000\n",
    "for i in range(iterations):\n",
    "    \n",
    "    # get data\n",
    "    batch_obs, batch_actions, batch_adv, batch_v_t, batch_old_action_prob = None, None, None, None, None\n",
    "    \n",
    "    for env_runner in env_runners:\n",
    "        obs, actions, rewards, dones, values, old_action_prob = env_runner.run(T)\n",
    "        adv, v_t = compute_advantage_and_value_targets(rewards, values, dones)\n",
    "    \n",
    "        # assemble data from the different runners \n",
    "        batch_obs = torch.stack(obs[:-1]) if batch_obs == None else torch.cat([batch_obs,torch.stack(obs[:-1])])\n",
    "        batch_actions = np.stack(actions[:-1]) if batch_actions is None else np.concatenate([batch_actions,np.stack(actions[:-1])])\n",
    "        batch_adv = torch.stack(adv) if batch_adv == None else torch.cat([batch_adv,torch.stack(adv)])\n",
    "        batch_v_t = torch.stack(v_t) if batch_v_t == None else torch.cat([batch_v_t,torch.stack(v_t)]) \n",
    "        batch_old_action_prob = torch.stack(old_action_prob[:-1]) if batch_old_action_prob == None else torch.cat([batch_old_action_prob,torch.stack(old_action_prob[:-1])])\n",
    "    \n",
    "    # load into dataset/loader\n",
    "    dataset = Batch_DataSet(batch_obs,batch_actions,batch_adv,batch_v_t,batch_old_action_prob)\n",
    "    dataloader = DataLoader(dataset, batch_size=minibatch_size, num_workers=0, shuffle=True)\n",
    "    \n",
    "    # update\n",
    "    for epoch in range(epochs):\n",
    "         \n",
    "        # sample minibatches\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # get data\n",
    "            obs, actions, adv, v_target, old_action_prob = batch\n",
    "            \n",
    "            adv = adv.squeeze(1)\n",
    "            # normalize adv values\n",
    "            #adv = ( adv - torch.mean(adv) ) / ( torch.std(adv) + 1e-8)\n",
    "            \n",
    "            # get policy actions probs for prob ratio & value prediction\n",
    "            pi, v = agent(obs)\n",
    "            # get the correct policy actions\n",
    "            pi = pi[range(minibatch_size),actions.long()]\n",
    "            \n",
    "            # probaility ratio r_t(theta)\n",
    "            probability_ratio = pi / old_action_prob\n",
    "            \n",
    "            # compute CPI\n",
    "            CPI = probability_ratio * adv\n",
    "            # compute clip*A_t\n",
    "            clip = torch.clamp(probability_ratio,1-eps,1+eps) * adv\n",
    "            \n",
    "            # policy loss | take minimum\n",
    "            L_CLIP = torch.mean(torch.min(CPI, clip))\n",
    "            \n",
    "            # value loss | mse\n",
    "            L_VF = torch.mean(torch.pow(v - v_target,2))\n",
    "            \n",
    "            loss = - L_CLIP + c1 * L_VF\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
